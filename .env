# Wikipedia / Dataset download
DATASET_URL=https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
DATASET_FILE=enwiki-latest-pages-articles.xml.bz2

# Train override â€” continue training existing model. Set 'false' to skip training (queries + upload)
MODEL_TRAIN=false

# Set checkpoint strategy to "streaming" (default .txt) or "serialized" (serialised .pkl file)
CORPUS_CHECKPOINT_STRATEGY=streaming

# Embedding model save path & config (MODEL_TYPE can also be FastText...)
MODEL_TYPE=Word2Vec
MODEL_NAME=word2vec_enwiki-latest-pages-articles

# Whether to resume training from an existing model (true) or start anew (false)
MODEL_RESUME=true

# Model hyperparameters
VECTOR_SIZE=100
WINDOW=5
MIN_COUNT=3
EPOCHS=10

# Vector DB integration
UPLOAD_TO_VECTORDB=true
VECTORDB_HOST=127.0.0.1
VECTORDB_PORT=6333
VECTORDB_COLLECTION=word2vec_enwiki-latest-pages-articles
